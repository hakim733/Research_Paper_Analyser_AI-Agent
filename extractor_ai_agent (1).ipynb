{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOKt5zNF9wE8",
        "outputId": "ed09e886-7d26-45ca-f20a-a16c72436ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: openalexapi in /usr/local/lib/python3.12/dist-packages (0.0.1a0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "üì• Downloading and extracting...\n",
            "üîç LLM extracting structured info...\n",
            "üìö Chunking text...\n",
            "\n",
            "üéØ PAPER ANALYSIS SUMMARY:\n",
            "{\n",
            "  \"title\": \"Attention Is All You Need\",\n",
            "  \"authors\": [\n",
            "    \"Ashish Vaswani\",\n",
            "    \"Noam Shazeer\",\n",
            "    \"Niki Parmar\",\n",
            "    \"Jakob Uszkoreit\",\n",
            "    \"Llion Jones\",\n",
            "    \"Aidan N. Gomez\",\n",
            "    \"\\u0141ukasz Kaiser\",\n",
            "    \"Illia Polosukhin\"\n",
            "  ],\n",
            "  \"abstract\": \"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\",\n",
            "  \"key_concepts\": [\n",
            "    \"Transformer\",\n",
            "    \"Attention Mechanism\",\n",
            "    \"Self-Attention\",\n",
            "    \"Multi-Head Attention\",\n",
            "    \"Scaled Dot-Product Attention\",\n",
            "    \"Sequence Transduction\",\n",
            "    \"Machine Translation\",\n",
            "    \"Recurrent Neural Networks\",\n",
            "    \"Convolutional Neural Networks\"\n",
            "  ],\n",
            "  \"methodology\": \"The Transformer model is based on a stack of identical layers, each consisting of two sub-layers: a multi-head self-attention mechanism and a simple, position-wise, fully connected feed-forward network. The model uses residual connections and layer normalization to facilitate training.\",\n",
            "  \"main_findings\": [\n",
            "    \"The Transformer model achieves state-of-the-art results in machine translation tasks, outperforming existing models by over 2 BLEU points.\",\n",
            "    \"The model is more parallelizable and requires significantly less time to train than existing models, achieving a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.\",\n",
            "    \"The Transformer model generalizes well to other tasks, including English constituency parsing, both with large and limited training data.\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "üìà ESTIMATED CIR METRICS:\n",
            "{\n",
            "  \"semantic_citations\": 0,\n",
            "  \"crossref_citations\": 10,\n",
            "  \"citations_final\": 10,\n",
            "  \"expected_field_avg\": 1.25,\n",
            "  \"novelty_score\": 1,\n",
            "  \"normalized_citations\": 0.8,\n",
            "  \"impact_norm\": 0.92,\n",
            "  \"estimated_CIR\": 0.88\n",
            "}\n",
            "\n",
            "üìä LLM-BASED UCR RESULTS:\n",
            "{\n",
            "  \"total_claims\": 3,\n",
            "  \"supported_claims\": 3,\n",
            "  \"unsupported_claims\": 0,\n",
            "  \"ucr_rate\": 0.0,\n",
            "  \"supported_examples\": [\n",
            "    \"The Transformer architecture introduced self-attention for sequence modeling\",\n",
            "    \"It eliminated recurrence and convolution, achieving state-of-the-art results in translation tasks\"\n",
            "  ],\n",
            "  \"unsupported_examples\": []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pdfplumber groq pydantic requests\n",
        "!pip install requests tqdm openalexapi\n",
        "\n",
        "import os, io, re, json, requests, pdfplumber\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "from groq import Groq\n",
        "\n",
        "# ====== CONFIG ======\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "MODEL = \"llama-3.3-70b-versatile\"\n",
        "url = \"https://arxiv.org/pdf/1706.03762\"  # Transformer paper\n",
        "# ====================\n",
        "\n",
        "\n",
        "# -------- LLM Backend --------\n",
        "class LLMBackend:\n",
        "    def __init__(self, model=MODEL):\n",
        "        self.client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "        self.model = model\n",
        "\n",
        "    def chat(self, prompt, system=None, temperature=0.2):\n",
        "        messages = []\n",
        "        if system:\n",
        "            messages.append({\"role\": \"system\", \"content\": system})\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model, messages=messages, temperature=temperature\n",
        "        )\n",
        "        return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "# -------- Schema --------\n",
        "class ResearchPaperAnalysis(BaseModel):\n",
        "    title: str\n",
        "    authors: List[str]\n",
        "    abstract: str\n",
        "    key_concepts: List[str]\n",
        "    methodology: str\n",
        "    main_findings: List[str]\n",
        "\n",
        "\n",
        "# -------- Main Processor --------\n",
        "class RAGResearchProcessorLLM:\n",
        "    def __init__(self, llm_backend: LLMBackend):\n",
        "        self.llm = llm_backend\n",
        "\n",
        "    def extract_document_text(self, url):\n",
        "        response = requests.get(url)\n",
        "        pdf_file = io.BytesIO(response.content)\n",
        "        full_text, pages = \"\", []\n",
        "        with pdfplumber.open(pdf_file) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                text = page.extract_text() or \"\"\n",
        "                full_text += text + \"\\n\"\n",
        "                pages.append({\n",
        "                    \"page_number\": i + 1,\n",
        "                    \"content\": text,\n",
        "                    \"word_count\": len(text.split()),\n",
        "                })\n",
        "        return full_text, pages\n",
        "\n",
        "    def analyze_research_paper(self, text):\n",
        "        prompt = f\"\"\"\n",
        "You are a precise research summarizer.\n",
        "Extract the following information from the paper text below.\n",
        "Respond ONLY with valid JSON ‚Äî no explanations.\n",
        "\n",
        "Keys:\n",
        "\"title\": string,\n",
        "\"authors\": list of author names,\n",
        "\"abstract\": string,\n",
        "\"key_concepts\": list of 5‚Äì10 technical terms,\n",
        "\"methodology\": string,\n",
        "\"main_findings\": list of 3‚Äì5 key findings.\n",
        "\n",
        "--- PAPER TEXT (truncated) ---\n",
        "{text[:10000]}\n",
        "--- END OF TEXT ---\n",
        "\"\"\"\n",
        "        raw = self.llm.chat(prompt)\n",
        "        match = re.search(r\"\\{.*\\}\", raw, re.S)\n",
        "        if match:\n",
        "            raw = match.group()\n",
        "        try:\n",
        "            data = json.loads(raw)\n",
        "        except Exception:\n",
        "            repair_prompt = f\"Fix and return valid JSON only:\\n\\n{raw}\"\n",
        "            repaired = self.llm.chat(repair_prompt)\n",
        "            try:\n",
        "                data = json.loads(repaired)\n",
        "            except Exception:\n",
        "                print(\"‚ö†Ô∏è Could not parse model output, returning fallback\")\n",
        "                data = {\n",
        "                    \"title\": \"Parsing Error\",\n",
        "                    \"authors\": [],\n",
        "                    \"abstract\": \"\",\n",
        "                    \"key_concepts\": [],\n",
        "                    \"methodology\": \"\",\n",
        "                    \"main_findings\": [],\n",
        "                }\n",
        "        return ResearchPaperAnalysis(**data)\n",
        "\n",
        "    def create_rag_chunks(self, pages, chunk_size=500):\n",
        "        chunks = []\n",
        "        for page in pages:\n",
        "            text = page[\"content\"]\n",
        "            if not text.strip():\n",
        "                continue\n",
        "            sentences = re.split(r\"[.!?]+\", text)\n",
        "            current = \"\"\n",
        "            for sent in sentences:\n",
        "                sent = sent.strip()\n",
        "                if len(current) + len(sent) < chunk_size:\n",
        "                    current += sent + \". \"\n",
        "                else:\n",
        "                    chunks.append({\n",
        "                        \"page\": page[\"page_number\"],\n",
        "                        \"content\": current.strip(),\n",
        "                        \"word_count\": len(current.split()),\n",
        "                    })\n",
        "                    current = sent + \". \"\n",
        "            if current:\n",
        "                chunks.append({\n",
        "                    \"page\": page[\"page_number\"],\n",
        "                    \"content\": current.strip(),\n",
        "                    \"word_count\": len(current.split()),\n",
        "                })\n",
        "        return chunks\n",
        "\n",
        "    def process_paper(self, url):\n",
        "        print(\"üì• Downloading and extracting...\")\n",
        "        text, pages = self.extract_document_text(url)\n",
        "        print(\"üîç LLM extracting structured info...\")\n",
        "        analysis = self.analyze_research_paper(text)\n",
        "        print(\"üìö Chunking text...\")\n",
        "        chunks = self.create_rag_chunks(pages)\n",
        "        return {\"paper_analysis\": analysis.model_dump(), \"chunks\": chunks}\n",
        "\n",
        "\n",
        "# -------- LLM-Based UCR Evaluator --------\n",
        "class LLMUCREvaluator:\n",
        "    def __init__(self, llm_backend: LLMBackend):\n",
        "        self.llm = llm_backend\n",
        "\n",
        "    def analyze_claim_support(self, generated_text: str, chunks: List[Dict]) -> Dict:\n",
        "        claims = [\n",
        "            s.strip()\n",
        "            for s in re.split(r\"[.!?]+\", generated_text)\n",
        "            if len(s.split()) > 5\n",
        "        ]\n",
        "        supported, unsupported = [], []\n",
        "        for claim in claims:\n",
        "            context = \"\\n\\n\".join(ch[\"content\"] for ch in chunks[:5])\n",
        "            prompt = f\"\"\"\n",
        "Determine if the following claim is supported by the given research context.\n",
        "\n",
        "CLAIM:\n",
        "{claim}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "Respond only with one word: SUPPORTED or UNSUPPORTED.\n",
        "\"\"\"\n",
        "            resp = self.llm.chat(prompt)\n",
        "            if \"SUPPORTED\" in resp.upper():\n",
        "                supported.append(claim)\n",
        "            else:\n",
        "                unsupported.append(claim)\n",
        "        total = len(claims)\n",
        "        return {\n",
        "            \"total_claims\": total,\n",
        "            \"supported_claims\": len(supported),\n",
        "            \"unsupported_claims\": len(unsupported),\n",
        "            \"ucr_rate\": len(unsupported) / total if total else 0,\n",
        "            \"supported_examples\": supported[:2],\n",
        "            \"unsupported_examples\": unsupported[:2],\n",
        "        }\n",
        "\n",
        "\n",
        "# -------- CIR Estimator --------\n",
        "class CIREstimator:\n",
        "    def __init__(self, llm_backend: LLMBackend):\n",
        "        self.llm = llm_backend\n",
        "\n",
        "    def fetch_citation_count(self, title: str) -> int:\n",
        "        try:\n",
        "            api = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={title}&limit=1&fields=citationCount\"\n",
        "            r = requests.get(api)\n",
        "            if r.status_code == 200:\n",
        "                data = r.json()\n",
        "                if data.get(\"data\"):\n",
        "                    return data[\"data\"][0].get(\"citationCount\", 0)\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Citation fetch failed:\", e)\n",
        "        return 0\n",
        "\n",
        "    def estimate_impact_and_relevance(self, abstract: str) -> Dict[str, float]:\n",
        "        prompt = f\"\"\"\n",
        "Rate the following research abstract on a scale of 0 to 1 for:\n",
        "(1) Scientific impact (importance of results)\n",
        "(2) Research novelty (originality of contribution)\n",
        "\n",
        "Return only JSON:\n",
        "{{\"impact\": value, \"novelty\": value}}\n",
        "\n",
        "Abstract:\n",
        "{abstract}\n",
        "\"\"\"\n",
        "        try:\n",
        "            raw = self.llm.chat(prompt)\n",
        "            match = re.search(r\"\\{.*\\}\", raw, re.S)\n",
        "            if match:\n",
        "                return json.loads(match.group())\n",
        "        except Exception:\n",
        "            pass\n",
        "        return {\"impact\": 0.5, \"novelty\": 0.5}\n",
        "\n",
        "    def compute_cir_score(self, title: str, abstract: str) -> Dict[str, Any]:\n",
        "        citations = self.fetch_citation_count(title)\n",
        "        scores = self.estimate_impact_and_relevance(abstract)\n",
        "        norm_citations = min(1.0, (1 + (citations / 100))) if citations < 100 else 1.0\n",
        "        impact = scores[\"impact\"]\n",
        "        novelty = scores[\"novelty\"]\n",
        "        cir = 0.5 * norm_citations + 0.3 * impact + 0.2 * novelty\n",
        "        return {\n",
        "            \"citations\": citations,\n",
        "            \"impact_score\": round(impact, 2),\n",
        "            \"novelty_score\": round(novelty, 2),\n",
        "            \"normalized_citations\": round(norm_citations, 2),\n",
        "            \"estimated_CIR\": round(cir, 2),\n",
        "        }\n",
        "\n",
        "\n",
        "    # ---------- CIR 2 ----------\n",
        "\n",
        "\n",
        "\n",
        "import math, time\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CIREstimator:\n",
        "    \"\"\"Semi-realistic CIR using Semantic Scholar + CrossRef + OpenAlex data.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_backend: LLMBackend):\n",
        "        self.llm = llm_backend\n",
        "\n",
        "    # ---------- Citation count ----------\n",
        "    def fetch_citations_semanticscholar(self, title: str) -> int:\n",
        "        try:\n",
        "            api = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={title}&limit=1&fields=citationCount\"\n",
        "            r = requests.get(api, timeout=15)\n",
        "            if r.status_code == 200:\n",
        "                data = r.json()\n",
        "                if data.get(\"data\"):\n",
        "                    return data[\"data\"][0].get(\"citationCount\", 0)\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è  Semantic Scholar fetch failed:\", e)\n",
        "        return 0\n",
        "\n",
        "    def fetch_citations_crossref(self, title: str) -> int:\n",
        "        \"\"\"Fallback citation count via CrossRef.\"\"\"\n",
        "        try:\n",
        "            r = requests.get(\n",
        "                \"https://api.crossref.org/works\",\n",
        "                params={\"query.title\": title, \"rows\": 1},\n",
        "                timeout=15\n",
        "            )\n",
        "            if r.status_code == 200:\n",
        "                items = r.json().get(\"message\", {}).get(\"items\", [])\n",
        "                if items:\n",
        "                    return items[0].get(\"is-referenced-by-count\", 0)\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è  CrossRef fetch failed:\", e)\n",
        "        return 0\n",
        "\n",
        "    # ---------- Field normalization ----------\n",
        "    def fetch_field_avg_from_openalex(self, title: str) -> float:\n",
        "        \"\"\"Rough expected citation baseline for the field/year.\"\"\"\n",
        "        try:\n",
        "            api = f\"https://api.openalex.org/works?filter=title.search:{title}&per-page=1\"\n",
        "            r = requests.get(api, timeout=15)\n",
        "            if r.status_code == 200:\n",
        "                data = r.json().get(\"results\", [])\n",
        "                if data and \"cited_by_count\" in data[0]:\n",
        "                    field = data[0][\"primary_topic\"][\"display_name\"] \\\n",
        "                            if data[0].get(\"primary_topic\") else \"Unknown\"\n",
        "                    year = data[0].get(\"publication_year\", 2020)\n",
        "                    # Estimate expected citations by age\n",
        "                    age = max(1, 2025 - year)\n",
        "                    return max(1.0, 10.0 / age)  # heuristic: older ‚Üí higher baseline\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è  OpenAlex fetch failed:\", e)\n",
        "        return 10.0  # default expected baseline\n",
        "\n",
        "    # ---------- Optional novelty estimation ----------\n",
        "    def estimate_novelty_llm(self, abstract: str) -> float:\n",
        "        prompt = f\"\"\"\n",
        "Rate the following research abstract for scientific novelty (0‚Äì1).\n",
        "Return JSON: {{\"novelty\": value}}\n",
        "Abstract: {abstract}\n",
        "\"\"\"\n",
        "        try:\n",
        "            raw = self.llm.chat(prompt)\n",
        "            m = re.search(r\"\\{.*\\}\", raw, re.S)\n",
        "            if m:\n",
        "                return json.loads(m.group())[\"novelty\"]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return 0.5\n",
        "\n",
        "    # ---------- Final composite CIR ----------\n",
        "    def compute_realistic_cir(self, title: str, abstract: str) -> Dict[str, Any]:\n",
        "        # Step 1: Gather data\n",
        "        ss_cit = self.fetch_citations_semanticscholar(title)\n",
        "        cr_cit = self.fetch_citations_crossref(title)\n",
        "        citations = max(ss_cit, cr_cit)\n",
        "        expected = self.fetch_field_avg_from_openalex(title)\n",
        "        novelty = self.estimate_novelty_llm(abstract)\n",
        "\n",
        "        # Step 2: Normalize\n",
        "        c_norm = min(1.0, citations / (expected * 10))  # field-normalized\n",
        "        i_norm = math.tanh(citations / (expected * 5))  # impact saturation curve\n",
        "\n",
        "        # Step 3: Combine\n",
        "        cir = 0.5 * c_norm + 0.3 * i_norm + 0.2 * novelty\n",
        "        return {\n",
        "            \"semantic_citations\": ss_cit,\n",
        "            \"crossref_citations\": cr_cit,\n",
        "            \"citations_final\": citations,\n",
        "            \"expected_field_avg\": round(expected, 2),\n",
        "            \"novelty_score\": round(novelty, 2),\n",
        "            \"normalized_citations\": round(c_norm, 2),\n",
        "            \"impact_norm\": round(i_norm, 2),\n",
        "            \"estimated_CIR\": round(cir, 2)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ======== RUN DEMO ========\n",
        "llm = LLMBackend()\n",
        "processor = RAGResearchProcessorLLM(llm)\n",
        "results = processor.process_paper(url)\n",
        "evaluator = LLMUCREvaluator(llm)\n",
        "\n",
        "# CIR estimation now runs AFTER results exist\n",
        "cir_estimator = CIREstimator(llm)\n",
        "cir_results = cir_estimator.compute_realistic_cir(\n",
        "    results[\"paper_analysis\"][\"title\"],\n",
        "    results[\"paper_analysis\"][\"abstract\"]\n",
        ")\n",
        "\n",
        "\n",
        "results[\"cir_estimation\"] = cir_results\n",
        "\n",
        "print(\"\\nüéØ PAPER ANALYSIS SUMMARY:\")\n",
        "print(json.dumps(results[\"paper_analysis\"], indent=2))\n",
        "\n",
        "print(\"\\nüìà ESTIMATED CIR METRICS:\")\n",
        "print(json.dumps(cir_results, indent=2))\n",
        "\n",
        "# Example generated claims for UCR check\n",
        "generated_summary = \"\"\"\n",
        "The Transformer architecture introduced self-attention for sequence modeling.\n",
        "It eliminated recurrence and convolution, achieving state-of-the-art results in translation tasks.\n",
        "However, it requires large computational resources for training.\n",
        "\"\"\"\n",
        "ucr = evaluator.analyze_claim_support(generated_summary, results[\"chunks\"])\n",
        "\n",
        "print(\"\\nüìä LLM-BASED UCR RESULTS:\")\n",
        "print(json.dumps(ucr, indent=2))\n"
      ]
    }
  ]
}